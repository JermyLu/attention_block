# attention_block
Pytorch版self-attention, multihead-attention, Atrous-attention, Local-attention工具箱，即插即用！


参考：苏剑林Keras版实现 https://github.com/bojone/attention
